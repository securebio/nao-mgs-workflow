// Helper function: Validate TSV structure by reading lines directly and splitting by tabs.
// This approach works even when files have only headers (no data rows),
// which can cause issues with nf-test's CSV parser.
def validateTsvStructure = { tsv_file, expected_col_count, expected_stage, expected_sample ->
    def lines = path(tsv_file).linesGzip

    // Verify header exists and has correct number of columns
    assert lines.size() >= 1 : "TSV file must have at least a header row"
    def header = lines[0].split("\t")
    assert header.size() == expected_col_count : "Expected ${expected_col_count} columns, got ${header.size()}"

    // If there are data rows, verify stage and sample columns
    if (lines.size() > 1) {
        def first_data_row = lines[1].split("\t", -1) // -1 keeps trailing empty strings
        def stage_idx = header.findIndexOf { it == "stage" }
        def sample_idx = header.findIndexOf { it == "sample" }

        assert stage_idx >= 0 : "Header must contain 'stage' column"
        assert sample_idx >= 0 : "Header must contain 'sample' column"
        assert first_data_row[stage_idx] == expected_stage : "Expected stage '${expected_stage}', got '${first_data_row[stage_idx]}'"
        assert first_data_row[sample_idx] == expected_sample : "Expected sample '${expected_sample}', got '${first_data_row[sample_idx]}'"
    }
}

nextflow_workflow {

    name "Test subworkflow QC"
    script "subworkflows/local/qc/main.nf"
    workflow "QC"
    tag "subworkflow"
    tag "qc"

    test("Should run without failures on paired data") {
        config "tests/configs/run.config"
        tag "expect_success"
        tag "interleaved"
        when {
            params {}
            workflow {
                """
                input[0] = Channel.of("test")
                    | combine(Channel.of("${projectDir}/test-data/tiny-index/reads/interleaved.fastq"))
                input[1] = "test"
                input[2] = Channel.of(false)
                """
            }
        }
        then {
            // Should run without failures
            assert workflow.success
            // Output is tuple(sample, basic, adapt, qbase, qseqs, lengths)
            def qc_tuple = workflow.out.qc[0]
            def tsv_basic = path(qc_tuple[1]).csv(sep: "\t", decompress: true)
            def fastq_in = path(workflow.out.test_input[0][1]).fastq
            def seqs_in = fastq_in.sequences
            def n_bases = seqs_in.collect{ it.length() }.sum()
            with (tsv_basic) {
                assert rowCount == 1
                assert columnCount == 18
                assert columns["n_reads_single"][0] == seqs_in.size()
                assert columns["n_reads_single"][0] == columns["n_read_pairs"][0] * 2
                assert Math.abs(columns["mean_seq_len"][0] - (n_bases / seqs_in.size())) < 0.001
                def n_bases_exp = Math.floor(columns["n_reads_single"][0] * columns["mean_seq_len"][0] / 100) * 100
                assert columns["n_bases_approx"][0] == n_bases_exp
                assert columns["stage"][0] == "test"
            }
            def stage = tsv_basic.columns["stage"][0]
            def sample = tsv_basic.columns["sample"][0]
            // Other TSVs should have expected structure and content
            validateTsvStructure(qc_tuple[2], 6, stage, sample)
            validateTsvStructure(qc_tuple[3], 5, stage, sample)
            validateTsvStructure(qc_tuple[4], 5, stage, sample)
            validateTsvStructure(qc_tuple[5], 5, stage, sample)
        }
    }

    test("Should run without failures on single-end data") {
        config "tests/configs/run.config"
        tag "expect_success"
        tag "single_end"
        when {
            params {}
            workflow {
                """
                input[0] = Channel.of("test")
                    | combine(Channel.of("${projectDir}/test-data/tiny-index/reads/R1.fastq"))
                input[1] = "test"
                input[2] = Channel.of(true)
                """
            }
        }
        then {
            // Should run without failures
            assert workflow.success
            // Output is tuple(sample, basic, adapt, qbase, qseqs, lengths)
            def qc_tuple = workflow.out.qc[0]
            def tsv_basic = path(qc_tuple[1]).csv(sep: "\t", decompress: true)
            def fastq_in = path(workflow.out.test_input[0][1]).fastq
            def seqs_in = fastq_in.sequences
            def n_bases = seqs_in.collect{ it.length() }.sum()
            with (tsv_basic) {
                assert rowCount == 1
                assert columnCount == 18
                assert columns["n_reads_single"][0] == seqs_in.size()
                assert Math.abs(columns["mean_seq_len"][0] - (n_bases / seqs_in.size())) < 0.001
                def n_bases_exp = Math.floor(columns["n_reads_single"][0] * columns["mean_seq_len"][0] / 100) * 100
                assert columns["n_bases_approx"][0] == n_bases_exp
                assert columns["n_read_pairs"][0] == ""
                assert columns["stage"][0] == "test"
            }
            def stage = tsv_basic.columns["stage"][0]
            def sample = tsv_basic.columns["sample"][0]
            // Other TSVs should have expected structure and content
            validateTsvStructure(qc_tuple[2], 6, stage, sample)
            validateTsvStructure(qc_tuple[3], 5, stage, sample)
            validateTsvStructure(qc_tuple[4], 5, stage, sample)
            validateTsvStructure(qc_tuple[5], 5, stage, sample)
        }
    }

}
