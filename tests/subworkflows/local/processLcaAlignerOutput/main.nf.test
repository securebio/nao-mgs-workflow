nextflow_workflow {

    name "Test Workflow PROCESS_LCA_ALIGNER_OUTPUT"
    script "subworkflows/local/processLcaAlignerOutput/main.nf"
    workflow "PROCESS_LCA_ALIGNER_OUTPUT"
    config "tests/configs/run.config"
    tag "subworkflow"
    tag "process_lca_aligner_output"

    def validateOutputStructure = { outputData, colsNoPrefix, colsAddPrefix, columnPrefix ->
        assert outputData.columnCount > 0
        assert outputData.rowCount > 0
        
        for (col in colsNoPrefix) {
            assert col in outputData.columns.keySet()
        }
        
        for (col in colsAddPrefix) {
            assert "${columnPrefix}${col}".toString() in outputData.columns.keySet()
        }
        
        assert outputData.columnCount == colsNoPrefix.size() + colsAddPrefix.size()
    }

    def validateJoinIntegrity = { outputSeqIds, lcaSeqIds, alignerSeqIds ->
        for (seqId in outputSeqIds) {
            assert seqId in lcaSeqIds
            assert seqId in alignerSeqIds
        }
    }

    def getPrimaryAlignmentSeqIds = { alignerData ->
        def primaryAlignerSeqIds = []
        for (int i = 0; i < alignerData.rowCount; i++) {
            if (alignerData.columns["classification"][i] == "primary") {
                primaryAlignerSeqIds.add(alignerData.columns["seq_id"][i])
            }
        }
        return primaryAlignerSeqIds.toSet()
    }

    def validateDataIntegrity = { outputData, lcaData, alignerData, colsNoPrefix, colsAddPrefix, columnPrefix ->
        for (int i = 0; i < outputData.rowCount; i++) {
            def seqId = outputData.columns["seq_id"][i]
            
            def lcaIndex = lcaData.columns["seq_id"].findIndexOf { it == seqId }
            
            def alignerIndex = -1
            for (int j = 0; j < alignerData.rowCount; j++) {
                if (alignerData.columns["seq_id"][j] == seqId && alignerData.columns["classification"][j] == "primary") {
                    alignerIndex = j
                    break
                }
            }
            
            for (col in colsNoPrefix) {
                if (col in lcaData.columns.keySet()) {
                    assert outputData.columns[col][i] == lcaData.columns[col][lcaIndex]
                }
            }
            
            for (col in colsAddPrefix) {
                if (col in alignerData.columns.keySet()) {
                    def prefixedCol = "${columnPrefix}${col}".toString()
                    assert outputData.columns[prefixedCol][i] == alignerData.columns[col][alignerIndex]
                }
            }
        }
    }

    test("Should run without failures on Bowtie2 output") {
        tag "expect_success"
        tag "bowtie2"
        when {
            params {
                col_keep_no_prefix = ["seq_id", "sample", "aligner_taxid_lca"]
                col_keep_add_prefix = ["genome_id_all"]
                column_prefix = "prim_align_"
            }
            workflow {
                """
                input[0] = Channel.of(["test", "${projectDir}/test-data/processLcaAlignerOutput/lca_bowtie.tsv"])
                input[1] = Channel.of(["test", "${projectDir}/test-data/processLcaAlignerOutput/tab_bowtie.tsv"])
                input[2] = params.col_keep_no_prefix
                input[3] = params.col_keep_add_prefix
                input[4] = params.column_prefix
                """
            }
        }

        then {
            assert workflow.success

            // Outputs are now tuple(sample, file), so access [1] for the file
            // Note: lca_tsv and aligner_tsv are plain TSV, viral_hits_tsv is gzipped
            def lcaData = path(workflow.out.lca_tsv[0][1]).csv(sep: "\t")
            def alignerData = path(workflow.out.aligner_tsv[0][1]).csv(sep: "\t")
            def outputData = path(workflow.out.viral_hits_tsv[0][1]).csv(sep: "\t", decompress: true)

            def colsNoPrefix = params.col_keep_no_prefix
            def colsAddPrefix = params.col_keep_add_prefix

            validateOutputStructure(outputData, colsNoPrefix, colsAddPrefix, params.column_prefix)

            def outputSeqIds = outputData.columns["seq_id"].toSet()
            def lcaSeqIds = lcaData.columns["seq_id"].toSet()
            def alignerSeqIds = alignerData.columns["seq_id"].toSet()
            validateJoinIntegrity(outputSeqIds, lcaSeqIds, alignerSeqIds)

            def primaryAlignerSeqIdsSet = getPrimaryAlignmentSeqIds(alignerData)
            def expectedSeqIds = primaryAlignerSeqIdsSet.intersect(lcaSeqIds)
            assert outputSeqIds == expectedSeqIds

            validateDataIntegrity(outputData, lcaData, alignerData, colsNoPrefix, colsAddPrefix, params.column_prefix)
        }
    }

    test("Should run without failures on Minimap2 output") {
        tag "expect_success"
        tag "minimap2"
        when {
            params {
                col_keep_no_prefix = ["seq_id", "sample", "aligner_taxid_lca"]
                col_keep_add_prefix = ["genome_id_all"]
                column_prefix = "prim_align_"
            }
            workflow {
                """
                input[0] = Channel.of(["test", "${projectDir}/test-data/processLcaAlignerOutput/lca_minimap.tsv"])
                input[1] = Channel.of(["test", "${projectDir}/test-data/processLcaAlignerOutput/tab_minimap.tsv"])
                input[2] = params.col_keep_no_prefix
                input[3] = params.col_keep_add_prefix
                input[4] = params.column_prefix
                """
            }
        }

        then {
            assert workflow.success

            // Outputs are now tuple(sample, file), so access [1] for the file
            // Note: lca_tsv and aligner_tsv are plain TSV, viral_hits_tsv is gzipped
            def lcaData = path(workflow.out.lca_tsv[0][1]).csv(sep: "\t")
            def alignerData = path(workflow.out.aligner_tsv[0][1]).csv(sep: "\t")
            def outputData = path(workflow.out.viral_hits_tsv[0][1]).csv(sep: "\t", decompress: true)

            def colsNoPrefix = params.col_keep_no_prefix
            def colsAddPrefix = params.col_keep_add_prefix

            validateOutputStructure(outputData, colsNoPrefix, colsAddPrefix, params.column_prefix)

            def outputSeqIds = outputData.columns["seq_id"].toSet()
            def lcaSeqIds = lcaData.columns["seq_id"].toSet()
            def alignerSeqIds = alignerData.columns["seq_id"].toSet()
            validateJoinIntegrity(outputSeqIds, lcaSeqIds, alignerSeqIds)

            def primaryAlignerSeqIdsSet = getPrimaryAlignmentSeqIds(alignerData)
            def expectedSeqIds = primaryAlignerSeqIdsSet.intersect(lcaSeqIds)
            assert outputSeqIds == expectedSeqIds

            validateDataIntegrity(outputData, lcaData, alignerData, colsNoPrefix, colsAddPrefix, params.column_prefix)
        }
    }


    test("Should handle empty input files") {
        tag "empty_input"
        tag "expect_success"

        when {
            params {
                col_keep_no_prefix = ["seq_id", "aligner_taxid_lca"]
                col_keep_add_prefix = ["genome_id_all"]
                column_prefix = "prim_align_"
            }
            workflow {
                """
                input[0] = Channel.of(["test", "${projectDir}/test-data/toy-data/empty_file.txt"])
                input[1] = Channel.of(["test", "${projectDir}/test-data/toy-data/empty_file.txt"])
                input[2] = params.col_keep_no_prefix
                input[3] = params.col_keep_add_prefix
                input[4] = params.column_prefix
                """
            }
        }

        then {
            assert workflow.success

            // Output is now tuple(sample, file), so access [1] for the file
            def outputFile = path(workflow.out.viral_hits_tsv[0][1])
            assert outputFile.exists()

            def outputLines = outputFile.linesGzip.size()
            assert outputLines == 0
        }
    }

    test("Should handle file with header only") {
        tag "empty_input"
        tag "header_only"
        tag "expect_success"

        when {
            params {
                col_keep_no_prefix = ["seq_id", "aligner_taxid_lca", "aligner_taxid_top",
                                      "aligner_length_normalized_score_mean", "aligner_taxid_lca_combined",
                                      "aligner_n_assignments_combined", "aligner_length_normalized_score_mean_combined",
                                      "aligner_taxid_lca_artificial", "aligner_n_assignments_artificial",
                                      "aligner_length_normalized_score_mean_artificial"]
                col_keep_add_prefix = ["genome_id_all", "taxid_all", "fragment_length",
                                       "best_alignment_score", "best_alignment_score_rev",
                                       "edit_distance", "edit_distance_rev", "ref_start",
                                       "ref_start_rev", "query_len", "query_len_rev",
                                       "query_seq", "query_seq_rev", "query_rc",
                                       "query_rc_rev", "query_qual", "query_qual_rev",
                                       "pair_status"]
                column_prefix = "prim_align_"
            }
            workflow {
                """
                input[0] = Channel.of(["test", "${projectDir}/test-data/processLcaAlignerOutput/header-only-lca.tsv"])
                input[1] = Channel.of(["test", "${projectDir}/test-data/processLcaAlignerOutput/header-only-bowtie2-sam-processed.tsv"])
                input[2] = params.col_keep_no_prefix
                input[3] = params.col_keep_add_prefix
                input[4] = params.column_prefix
                 """
            }
        }

        then {
            assert workflow.success

            // Output is now tuple(sample, file), so access [1] for the file
            def outputFile = path(workflow.out.viral_hits_tsv[0][1])
            assert outputFile.exists()

            def outputLines = outputFile.linesGzip
            assert outputLines.size() == 1

            def colsNoPrefix = params.col_keep_no_prefix
            def colsAddPrefix = params.col_keep_add_prefix

            for (col in colsNoPrefix) {
                outputLines.contains(col)
            }

            for (col in colsAddPrefix) {
                outputLines.contains("${params.column_prefix}${col}".toString())
            }
        }
    }
}
