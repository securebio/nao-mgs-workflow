nextflow_workflow {

    name "Test workflow SPLIT_VIRAL_TSV_BY_SELECTED_TAXID"
    script "subworkflows/local/splitViralTsvBySelectedTaxid/main.nf"
    workflow "SPLIT_VIRAL_TSV_BY_SELECTED_TAXID"
    config "tests/configs/downstream.config"
    tag "subworkflow"
    tag "downstream"
    tag "split_viral_tsv_by_species"

    test("Should fail when input TSV contains duplicate read IDs") {
        tag "expect_failed"
        tag "duplicate_input"
        when {
            params {
            }
            workflow {
                '''
                input[0] = Channel.of("test")
                    | combine(Channel.of("${projectDir}/test-data/splitViralTsvBySelectedTaxid/input_duplicate.tsv")) // groups
                input[1] = Channel.of("${params.ref_dir}/results/total-virus-db-annotated.tsv.gz") // db
                '''
            }
        }
        then {
            // Should run without failures
            assert workflow.failed
            assert workflow.exitStatus == 1
            assert workflow.errorReport.contains("ValueError: Duplicate value found in field seq_id")
        }
    }

    test("Should handle empty partition files (no data rows)") {
        tag "expect_success"
        tag "empty_partition"
        when {
            params {
            }
            workflow {
                '''
                input[0] = Channel.of("test")
                    | combine(Channel.of("${projectDir}/test-data/splitViralTsvBySelectedTaxid/input_empty.tsv")) // groups
                input[1] = Channel.of("${params.ref_dir}/results/total-virus-db-annotated.tsv.gz") // db
                '''
            }
        }
        then {
            // Should run without failures even when input TSV has no data rows
            assert workflow.success
            // TSV output should be empty (partition_empty_* filtered out)
            assert workflow.out.tsv.collect{it[1]}.flatten().size() == 0
            // FASTQ output should be empty
            assert workflow.out.fastq.collect{it[1]}.flatten().size() == 0
        }
    }

    test("Should run without failures on valid input data") {
        tag "expect_success"
        tag "valid_data"
        when {
            params {
            }
            workflow {
                '''
                input[0] = Channel.of("test")
                    | combine(Channel.of("${projectDir}/test-data/splitViralTsvBySelectedTaxid/input_valid.tsv")) // groups
                input[1] = Channel.of("${params.ref_dir}/results/total-virus-db-annotated.tsv.gz") // db
                '''
            }
        }
        then {
            // Should run without failures
            assert workflow.success
            // Sorting should work as expected
            def tabs_in = workflow.out.test_in.collect{path(it[1]).csv(sep: "\t")}
            def tabs_sort = workflow.out.test_sort.collect{path(it[1]).csv(sep: "\t")}
            assert tabs_sort.size() == tabs_in.size()
            for (int i=0; i < tabs_in.size(); i++) {
                assert tabs_sort[i].rowCount == tabs_in[i].rowCount
                assert tabs_sort[i].columnCount == tabs_in[i].columnCount
                assert tabs_sort[i].columns["aligner_taxid"].collect{ it.toString() } == tabs_in[i].columns["aligner_taxid"].collect{ it.toString() }.toSorted()
            }
            // Joined DB should have expected contents
            def tabs_join = workflow.out.test_join.collect{path(it[1]).csv(sep: "\t",decompress: true)}
            def tab_db = path(workflow.out.test_db[0]).csv(sep: "\t", decompress: true)
            assert tabs_join.size() == tabs_sort.size()
            for (int i=0; i < tabs_sort.size(); i++) {
                assert tabs_join[i].rowCount == tabs_sort[i].rowCount
                assert tabs_join[i].columnNames == tabs_sort[i].columnNames + ["taxid_species"]
                for (c in tabs_join[i].columnNames) {
                    if (c in tabs_sort[i].columnNames) {
                        assert tabs_sort[i].columns[c] == tabs_join[i].columns[c]
                    } else {
                        assert c in tab_db.columnNames
                    }
                }
            }
            // Partition should produce expected number of output DBs
            def n_parts_total = 0
            def n_parts_exp
            def n_parts_obs
            for (int i=0; i < tabs_join.size(); i++) {
                n_parts_exp = tabs_join[i].columns["taxid_species"].unique().size()
                n_parts_obs = workflow.out.test_part[i][1].size()
                assert n_parts_exp == n_parts_obs
                n_parts_total += n_parts_obs
            }
            assert workflow.out.tsv.collect{it[1].size()}.sum() == n_parts_total
            // FASTQ extraction and merging should work as expected
            def tabs_part_2 = workflow.out.tsv.collectMany{it[1]}.collect{path(it).csv(sep: "\t",decompress: true)}
            def fastqs_interleaved = workflow.out.fastq.collectMany{it[1]}.collect{path(it).fastq}
            for (int i=0; i < tabs_part_2.size(); i++) {
                assert tabs_part_2[i].rowCount == fastqs_interleaved[i].sequences.size() / 2
            }
        }
    }

}
